{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af52bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78eb4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "full_data = pd.read_csv('data/full_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4600c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt35-reasoning_multitarget_nonotes.json.txt\n",
      "gpt35-reasoning_multitarget.txt\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "file_prefix = 'gpt35-reasoning'\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    if file.startswith(file_prefix) and file.endswith('.txt') and 'multitarget' in file:\n",
    "        print(file)\n",
    "        with open(file, 'r') as f:\n",
    "            \n",
    "            data = f.readlines()\n",
    "            if file_prefix!= 'gpt35-reasoning':\n",
    "                temp = [x.strip() for x in data]\n",
    "                \n",
    "                print(len(temp))\n",
    "                start = -1\n",
    "                fn = file.split('.')[0]\n",
    "                results[fn] = []\n",
    "                for i in range(len(temp)):\n",
    "                    if (temp[i].startswith('Yes') or temp[i].startswith('No') or temp[i].startswith('Unclear')):\n",
    "                        if start >= 0:\n",
    "                            results[fn].append('\\n'.join(temp[start:i]).strip())\n",
    "                        start = i\n",
    "    #                 else:\n",
    "    #                     print(temp[i])\n",
    "                results[fn].append('\\n'.join(temp[start:i]))\n",
    "            else:\n",
    "                results[file.split('.')[0]] = [x.strip() for x in ''.join(data).split('\\n-------------------------------------\\n')]\n",
    "#                 results[file.split('.')[0]] = [x.strip().split('If Yes, explain why.')[-1] for x in ''.join(data).split('\\n--------------\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee3a8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2204], 2204)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in results.values()], len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "824d47de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gpt35-reasoning_multitarget'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del results['gpt35-reasoning_multitarget_nonotes']\n",
    "\n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3681aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did he kill your sex life? Women are objects so you can just buy another one anyways.\n",
      "gpt35-reasoning_multitarget Yes, this post is a violation of the policies.\n",
      "\n",
      "1. CC_319: This post promotes hatred against the identifiable group of \"Age\" by expressing a strong negative sentiment towards children and young people. It implies that they are disgusting and vile, which can be seen as an expression of contempt and ill-treatment based on age.\n",
      "\n",
      "2. HR_AB: This post is likely to expose children and young people to hatred or contempt based on their age. It devalues and dehumanizes them\n",
      "Counter({0: 1548, 2: 653, -1: 3})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "hallucinations = 0\n",
    "print(full_data.iloc[0]['text'])\n",
    "for k, v in results.items():\n",
    "    if len(full_data) < len(v):\n",
    "        res = v[-len(full_data):]\n",
    "    else:\n",
    "        res = v\n",
    "        \n",
    "    print(k,res[1])\n",
    "    predictions = []\n",
    "    bag_of_words = []\n",
    "    for r in res:\n",
    "        coarse = (2 if r.strip().startswith('Yes') else (1 if r.strip().startswith('Unclear') else (0 if r.strip().startswith('No') else -1)))\n",
    "        if coarse == -1:\n",
    "            hallucinations += 1\n",
    "            if 'Yes' in r.strip():\n",
    "                coarse = 2\n",
    "            elif 'Unclear' in r.strip():\n",
    "                coarse = 1\n",
    "            elif 'No' in r.strip():\n",
    "                coarse = 0\n",
    "            else:\n",
    "                coarse = -1\n",
    "        \n",
    "        words = set(word_tokenize(r.lower()))\n",
    "        \n",
    "#         print(r, words)\n",
    "        predictions.append(coarse)\n",
    "        bag_of_words.append(words)\n",
    "        \n",
    "    print(Counter(predictions))\n",
    "    \n",
    "#     hallucinations += Counter(predictions[k])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ecb9895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " ',',\n",
       " '.',\n",
       " '1.',\n",
       " '2.',\n",
       " ':',\n",
       " '``',\n",
       " 'a',\n",
       " 'against',\n",
       " 'age',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'based',\n",
       " 'be',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cc_319',\n",
       " 'children',\n",
       " 'contempt',\n",
       " 'dehumanizes',\n",
       " 'devalues',\n",
       " 'disgusting',\n",
       " 'expose',\n",
       " 'expressing',\n",
       " 'expression',\n",
       " 'group',\n",
       " 'hatred',\n",
       " 'hr_ab',\n",
       " 'identifiable',\n",
       " 'ill-treatment',\n",
       " 'implies',\n",
       " 'is',\n",
       " 'it',\n",
       " 'likely',\n",
       " 'negative',\n",
       " 'of',\n",
       " 'on',\n",
       " 'or',\n",
       " 'people',\n",
       " 'policies',\n",
       " 'post',\n",
       " 'promotes',\n",
       " 'seen',\n",
       " 'sentiment',\n",
       " 'strong',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'they',\n",
       " 'this',\n",
       " 'to',\n",
       " 'towards',\n",
       " 'vile',\n",
       " 'violation',\n",
       " 'which',\n",
       " 'yes',\n",
       " 'young'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb84593d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, this post is a violation of the policies.\\n\\n1. It violates CC_319 as it communicates a statement that promotes hatred against immigrants, which is likely to lead to a breach of the peace.\\n2. It violates HR_BC as it displays a statement that indicates discrimination and exposes immigrants to hatred or contempt based on their immigration status.\\n3. It violates HR_NW as it expresses discrimination and incites others to discriminate against immigrants based on their nationality, ethnic origin, and immigration'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(results.values())[0][42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d06a6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013611615245009074"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucinations/sum([len(predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4c9f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It’s what sore losers and cheechakos call Senator Dan Sullivan. It’s sickening that women have the right to vote. Women are sore losers.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.iloc[5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d4816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "legal_defs = pd.read_csv('legal definitions.csv')\n",
    "legal_labels = legal_defs['promptName'].to_list()\n",
    "defs = pd.read_csv('data/definitions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce719259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC_318',\n",
       " 'CC_319',\n",
       " 'HR_AB',\n",
       " 'HR_BC',\n",
       " 'HR_NW',\n",
       " 'HR_SA',\n",
       " 'TOS_MET',\n",
       " 'TOS_RED',\n",
       " 'TOS_TK',\n",
       " 'TOS_TWT',\n",
       " 'TOS_YT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db71fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2109, 1: 95})\n",
      "Counter({0: 1659, 1: 545})\n",
      "Counter({0: 1727, 1: 477})\n",
      "Counter({0: 1972, 1: 232})\n",
      "Counter({0: 2140, 1: 64})\n",
      "Counter({0: 2169, 1: 35})\n",
      "Counter({0: 2119, 1: 85})\n",
      "Counter({0: 2142, 1: 62})\n",
      "Counter({0: 2111, 1: 93})\n",
      "Counter({0: 2107, 1: 97})\n",
      "Counter({0: 2125, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, hamming_loss\n",
    "fine = []\n",
    "pred = []\n",
    "\n",
    "scores = [[],[],[]]\n",
    "for i in range(len(legal_labels)):\n",
    "    temp = []\n",
    "    \n",
    "#     for j in range(len(bag_of_words)):\n",
    "#         r = bag_of_words[j]\n",
    "#         if legal_labels[i] in r:\n",
    "#             temp.append(1)\n",
    "#         else:\n",
    "#             temp.append(0)\n",
    "    temp = [(1 if legal_labels[i].lower() in x else 0) for x in bag_of_words]\n",
    "    pred.append(temp)        \n",
    "\n",
    "    finegrained_lab = [(1 if full_data.iloc[j]['label'] == 'Meaning unclear' \n",
    "                        else (2 if (not pd.isna(full_data.iloc[j]['finegrained']) and legal_labels[i].lower() in full_data.iloc[j]['finegrained'].lower()) \n",
    "                         else 0)) \n",
    "                       for j in range(len(full_data))]\n",
    "\n",
    "\n",
    "\n",
    "    fine.append([(1 if x > 1 else 0) for x in finegrained_lab])\n",
    "    print(Counter(temp))\n",
    "#         print(f1_score(finegrained_lab, predictions[r], average='micro'))\n",
    "#         print(f1_score(finegrained_lab, predictions[r], average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c122ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gpt35-reasoning_multitarget'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bce6104e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse = [[] for _ in range(len(full_data))]\n",
    "\n",
    "mapping = {'Violates': 2, 'Meaning unclear': 1, 'Does not violate': 0}\n",
    "\n",
    "coarse_lab = [mapping[x] for x in full_data['label'].to_list()]\n",
    "\n",
    "# for r in predictions.keys():\n",
    "# name = '_'.join(r.split('_')[-2:]).split('.')[0]\n",
    "# print(name)\n",
    "# if name in legal_labels:\n",
    "#     print(len(predictions))\n",
    "#     for i in range(len(predictions)):\n",
    "#         coarse[i].append(predictions[i])\n",
    "            \n",
    "\n",
    "            \n",
    "coarse_pred = predictions\n",
    "\n",
    "# coarse_pred = [(1 if x[1] > 4 else sorted(x.items(), key=lambda x: x[1], reverse=True)[0][0]) for x in coarse]\n",
    "coarse_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f173e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/14cfl/anaconda3/envs/pt2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/14cfl/anaconda3/envs/pt2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.315280105763461,\n",
       " 0.3099219563058197,\n",
       " 0.43723365589932306,\n",
       " 0.7699637023593466)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(coarse_lab, coarse_pred, average='macro'), precision_score(coarse_lab, coarse_pred, average='macro'), recall_score(coarse_lab, coarse_pred, average='macro'), f1_score(coarse_lab, coarse_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4002e624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22255584152487354,\n",
       " 0.33807799079794143,\n",
       " 0.3935956264292096,\n",
       " 0.338929219600726)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = np.random.randint(3, size=len(coarse_pred))\n",
    "\n",
    "f1_score(coarse_lab, rand, average='macro'), precision_score(coarse_lab, rand, average='macro'), recall_score(coarse_lab, rand, average='macro'), f1_score(coarse_lab, rand, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ace1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine = np.transpose(np.array(fine))\n",
    "pred = np.transpose(np.array(pred))\n",
    "\n",
    "rand = np.random.randint(2, size=pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ab785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/14cfl/anaconda3/envs/pt2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.15837615752601894,\n",
       " 0.13381075565978529,\n",
       " 0.2693363240005093,\n",
       " 0.2183730281472317)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(fine, pred, average='macro'), precision_score(fine, pred, average='macro'), recall_score(fine, pred, average='macro'), f1_score(fine, pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceac8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/14cfl/anaconda3/envs/pt2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.04284151576735985,\n",
       " 0.05641996514274191,\n",
       " 0.0372862832368277,\n",
       " 0.10153412880752982)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(fine, rand, average='macro'), precision_score(fine, rand, average='macro'), recall_score(fine, rand, average='macro'), f1_score(fine, rand, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('words.json', 'w') as f:\n",
    "    f.write(json.dumps(list(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\n",
    "         'Race (colour, creed)': ['actual and perceived race', 'colour', 'creed', 'race', 'race or perceived race'],\n",
    "         'National or ethnic origin (nationality, ethnicity, ancestry)': ['ancestry', 'national', 'ethnic origin', 'ethnicity', 'place of origin', 'national origin', 'national or ethnic origin', 'nationality', 'indigenous identity'],\n",
    "         'Political belief/association': ['political association', 'political belief'],\n",
    "         'Sex/Gender identity or expression': ['gender', 'gender identity', 'gender identity and expression', 'gender identity or expression', 'sex', 'sex/gender'],\n",
    "         'Religion/Religious beliefs': ['religion', 'religious affiliation','religious beliefs' 'religion/religious'],\n",
    "         'Sexual orientation': ['sexual orientation'],\n",
    "         'Social condition': ['social condition'],\n",
    "         'Immigration status': ['immigration status', 'immigration'],\n",
    "         'Source of income': ['source of income'],\n",
    "         'Age': ['age'],\n",
    "         'Physical or mental disability': ['physical or mental disability', 'mental or physical disability', 'physical disability', 'mental disability', 'disability', 'pregnancy or disability', 'adhd'],\n",
    "         'Family affiliation': ['family affiliation', 'caste'],\n",
    "         'Conviction that is subject to a pardon or record suspension': ['conviction that is subject to a pardon or record suspension'],\n",
    "         'Receipt of public assistance': ['receipt of public assistance'],\n",
    "         'Serious disease': ['serious disease'],\n",
    "         'Family status': ['family status'],\n",
    "        'Pregnancy': ['pregnancy'],\n",
    "    'Victims of a major violent event and their families/kin': ['victims of a major violent event and their kin', 'victims of a major event and their families'],\n",
    "    'Veteran status': ['veteran status'],\n",
    "    'Marital status': ['marital status']\n",
    "}\n",
    "\n",
    "\n",
    "# groups = {x: k for k, v in groups.items() for x in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e02596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('groups_map.json', 'r') as f:\n",
    "    groups_new = json.loads(f.readline())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbd9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def jaccard(set1, set2):\n",
    "    return len(set1.intersection(set2))/len(set1.union(set2))\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(len(bag_of_words)):\n",
    "    y = set((literal_eval(full_data.iloc[i]['target_group']) if not pd.isna(full_data.iloc[i]['target_group']) else []))\n",
    "   \n",
    "    y_hat = set(([groups_new[x] for x in bag_of_words[i] if x in groups_new] if coarse_pred[i] == 2 else []))\n",
    "    print(y, y_hat)\n",
    "    if len(y) != 0 and len(y_hat) != 0:\n",
    "        scores.append(jaccard(y, y_hat))\n",
    "        print(scores[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5f8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_groups = [set(literal_eval(x)) for x in legal_defs['protected_groups_cleaned'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.intersection(*cleaned_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3037e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defea361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups_new_new = {}\n",
    "\n",
    "for k, v in groups_new.items():\n",
    "    if v in groups_new_new:\n",
    "        groups_new_new[v].append(k)\n",
    "    else:\n",
    "        groups_new_new[v] = [k]\n",
    "groups_new_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "base_url = 'http://api.conceptnet.io/'\n",
    "\n",
    "roots = set([x for y in groups.values() for x in y])\n",
    "# to_search = {}\n",
    "\n",
    "for r in tqdm(word_freq):\n",
    "    if r[0] in groups_new:\n",
    "        pass\n",
    "    else:\n",
    "        res = requests.get(base_url + '/c/en/{0}'.format(r[0].replace(' ', '_'))).json()\n",
    "\n",
    "        new_words = set([x['start']['label'] for x in res['edges']] + [x['end']['label'] for x in res['edges']])\n",
    "\n",
    "        if len(new_words.intersection(roots)) > 0:\n",
    "            print(r)\n",
    "            intersect = new_words.intersection(roots)\n",
    "            print(intersect)\n",
    "            for i in list(intersect):\n",
    "                if i in to_search:\n",
    "                    to_search[i].append(r[0])\n",
    "                else:\n",
    "                    to_search[i] = [r[0]]\n",
    "#     print([(x['start']['label'], x['end']['label']) for x in res['edges'] if x['end']['language'] == 'en' and x['start']['language'] == 'en' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48073c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "groups = copy.deepcopy(groups_new)\n",
    "groups_new = {}\n",
    "\n",
    "for k, v in groups.items():\n",
    "    print(k)\n",
    "    groups_new[k] = v\n",
    "    if k in to_search:\n",
    "        print(to_search)\n",
    "        for w in to_search[k]:\n",
    "            groups_new[w] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a02bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "words_count = Counter([x for y in bag_of_words for x in y])\n",
    "\n",
    "stp = set(stopwords.words('english'))\n",
    "word_freq = [x for x in words_count.items() if x[0] not in stp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_freq, key=lambda x: x[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
